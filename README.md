```markdown

![image](https://github.com/user-attachments/assets/8d5e0c61-dae6-40f9-9ae2-aa4e52a55807)

# Agentic Story Pipeline

A modular, agent-based pipeline for interactive story generation with humanâ€“LLM collaboration. This project orchestrates multiple â€œagentsâ€ (Outline, Background, Persona, Plot, Feedback, Revision, Synthesis, Verification), each backed by a shared LLM client, to produce and refine narrative drafts.

---

## ğŸš€ Features

- **Modular Agents**  
  - Outline â†’ Background â†’ Persona â†’ Plot â†’ Feedback â†’ Revision â†’ Synthesis â†’ Verification  
  - Each agent in its own package, with separate prompt templates (YAML).

- **Human-in-the-Loop**  
  - Inspect & edit after every LLM step via `prompt_edit()`.
  - Collect both LLM and human feedback before revision.

- **Configurable Generation**  
  - Centralized settings in `configs/api_config.yml` (model, endpoint, timeouts, sampling).
  - `.env` support via `python-dotenv`.

- **Streaming & Batch Modes**  
  - Support for streaming or blocking LLM responses.
  - Automatic handling of JSON and NDJSON formats.

- **Testing & CI**  
  - Unit tests for every agent and client in `tests/`.
  - Example GitHub Actions workflow for automated linting & `pytest`.

---

## ğŸ—‚ï¸ Folder Structure

```

agentic\_story/
â”œâ”€â”€ configs/           # YAML configs (api, logging, generation defaults)
â”œâ”€â”€ prompts/           # LLM â€œsystemâ€ & â€œuserâ€ templates (one YAML per agent)
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ clients/       # LLMClient implementation
â”‚   â”œâ”€â”€ agents/        # Agents: outline, plot, feedback, revision, â€¦
â”‚   â”œâ”€â”€ pipeline/      # `run_pipeline.py` orchestrator
â”‚   â””â”€â”€ utils/         # Shared helpers (I/O, logging, parsing)
â”œâ”€â”€ scripts/           # Convenience scripts (start\_local.sh, deploy.sh)
â”œâ”€â”€ data/              # Examples & output
â”œâ”€â”€ tests/             # Unit & integration tests
â”œâ”€â”€ .env               # Local environment variables (not checked in)
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md          # This file

````

---

## âš™ï¸ Installation

1. **Clone the repo**  
   ```bash
   git clone https://github.com/your-org/agentic_story.git
   cd agentic_story
````

2. **Create & activate a virtual environment**

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies**

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

---

## ğŸ“ Configuration

1. **Copy `.env.example` to `.env`** (create if missing):

   ```dotenv
   # .env
   LLM_API_URL=https://llm.srv.webis.de/api/chat
   LLM_MODEL=default
   API_CONFIG_PATH=agentic_story/configs/api_config.yml
   ```

2. **Edit `configs/api_config.yml`** to tweak:

   * `llm_api.url` / `model` / `timeout` / `stream`
   * `generation.max_tokens` / `temperature` / `top_p` / `frequency_penalty` / `presence_penalty`

3. **(Optional) Logging**

   * Configure `configs/logging.yml` to adjust log levels, handlers, and formatters.

---

## â–¶ï¸ Usage

Run the full interactive pipeline:

```bash
# via start_local.sh (exports env & starts pipeline)
./scripts/start_local.sh

# or manually:
python3 modules/pipeline/run_pipeline.py
```

1. Enter your story idea.
2. Review & optionally edit each LLM-generated step: outline, draft, feedback.
3. Provide free-form human feedback.
4. Receive the final revised story.

---

## âœ… Testing & CI

* **Run tests locally**

  ```bash
  pytest --maxfail=1 --disable-warnings -q
  ```

* **GitHub Actions**
  A workflow at `.github/workflows/ci.yml` installs dependencies and runs your test suite on every push & PR to `main`.

---

## ğŸ¤ Contributing

1. Fork the repo & create a feature branch.
2. Add or update tests in `tests/`.
3. Submit a PR with a clear description of your changes.

---

## ğŸ“„ License

This project is licensed under the MIT License. See [LICENSE](./LICENSE) for details.

```

Save this as **`README.md`** at your project root. Feel free to adjust URLs, badge images, or any section to match your organizationâ€™s conventions.
```
### TODO
test the params pass to LLM , its behavious record
create test case for workspace
create some agentic diagram
improve logging
feedback and metric addition
